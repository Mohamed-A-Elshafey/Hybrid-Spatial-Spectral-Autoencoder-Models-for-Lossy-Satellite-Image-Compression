{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "e43a0dd3be974735bccd7fe0883333da",
            "f8e72e79db544ed7ad80b92dea6d3068",
            "71023e35c033447f82fc58a491b06632",
            "aa8f9f76774b4f20be0dc46539e3e822",
            "fc056bd451774ae0b84a5bb0eeb30098",
            "1da7b4448a774b2ab80d736e85f46b2f",
            "e58b9957e5164565a91464b534cd6d50",
            "bf6ca18ec5124c39be67a6f504e4a10e",
            "dc2476b5c19941ada1a10af09eab8d07",
            "d5a5d5c658b547ae807d3b012862ed7a",
            "6aa1526968b140e9ae84003f0226680c",
            "2d68ec7795fe4cac85c9a7cd1fdc875e",
            "9abe5b7023bc4aa8b24f083725f0a6be",
            "6435fa784312463f8940ba26c4701c76",
            "e8b56274b78e485cbb430217d9aa06f1",
            "65d27d5c5a09450bb39e4679ee7a0232",
            "df32d6e647cc403fa97d566d4229bcf0",
            "c60c3d84598743b3b1fb0ed75d655417",
            "e68b074151cc44118abdd0c040c82956",
            "075def203cae4a61aafc55b8cd556228",
            "39a67fe5d07c4bd99390985175ca84aa",
            "b62f0f0b91644247b56fbd67c20ae58b",
            "7f923fa744fe4b93a17fc77b98ece0c1",
            "adea7a9694614e9eac8b03f1564844b2",
            "350689e7b27c434f9ffd0ea9b2e9b2e4",
            "bacc3a75e2744e0a8d2d77f982fa4ddb",
            "87150937f8d0490ea47e7ac611118ce5",
            "361114d416bc4bbd832ea09078f5ac5a",
            "de41e328fe5141d39ab2f802a3f1c618",
            "995292da6a5744828e48cba9b724f767",
            "a32045ee42874f329cb09cbf4bd27e46",
            "7584db6b27a04a73a20df696c7146cf6",
            "924be73919e34089a1ee317827a8a1b4"
          ]
        },
        "id": "3N2VQIpmtQ6_",
        "outputId": "04a74898-78f3-42aa-f639-cc0a19c3a825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 1.93 GiB (download: 1.93 GiB, generated: Unknown size, total: 1.93 GiB) to /root/tensorflow_datasets/eurosat/all/2.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e43a0dd3be974735bccd7fe0883333da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d68ec7795fe4cac85c9a7cd1fdc875e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f923fa744fe4b93a17fc77b98ece0c1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import sys\n",
        "\n",
        "#%%\n",
        "#**Load EuroSat dataset and split the data into train 70%, validation 20%, test 10%**\n",
        "\n",
        "ds_train, ds_val, ds_test = tfds.load('eurosat/all',\n",
        "                               split=['train[:70%]', 'train[70%:90%]', 'train[90%:]'],\n",
        "                               as_supervised = True)\n",
        "\n",
        "#We need to use mapping to axis only the images since the dataset has lots of information\n",
        "#in addition to labelling\n",
        "\n",
        "train = ds_train.map(lambda image, label: image)\n",
        "val = ds_val.map(lambda image, label: image)\n",
        "test = ds_test.map(lambda image, label: image)\n",
        "\n",
        "# make the dataset \"images\" iterable\n",
        "train_np = tfds.as_numpy(train)\n",
        "val_np = tfds.as_numpy(val)\n",
        "test_np = tfds.as_numpy(test)\n",
        "\n",
        "# iterate over the images and save it to a list\n",
        "x_train = []\n",
        "x_val = []\n",
        "x_test = []\n",
        "\n",
        "for ex in train_np:\n",
        "  #print (ex.shape)\n",
        "  x_train.append(ex)\n",
        "\n",
        "for ex in val_np:\n",
        "  #print (ex.shape)\n",
        "  x_val.append(ex)\n",
        "\n",
        "for ex in test_np:\n",
        "  #print (ex.shape)\n",
        "  x_test.append(ex)\n",
        "\n",
        " # Convert List to array\n",
        "x_train = np.array(x_train)\n",
        "x_val = np.array(x_val)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "rows, cols, channels = ex.shape\n",
        "print(ex.shape)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Import the 'os' module\n",
        "import os\n",
        "\n",
        "# Create the directory path for the saved weights\n",
        "weights_dir = '/content/gdrive/MyDrive/badr13_weights'\n",
        "weights_path = os.path.join(weights_dir, 'hybrid_depth_CNN_ATTENTION_multispectral_weights.h5')\n",
        "\n",
        "\n",
        "#%%\n",
        "# Hybrid parallel (CNN-attention mechanism) Autoencoder archeticture full compressor\n",
        "#rows, cols, channels = 64,64,13\n",
        "input_img = keras.Input(shape=(rows, cols, channels))\n",
        "###############################################################################\n",
        "# Spectral encoder path with attention mechanism for spectral feature extraction\n",
        "spectral_encoder = layers.Conv2D(256, (3, 3), activation='PReLU', padding='same')(input_img)\n",
        "spectral_encoder = layers.BatchNormalization()(spectral_encoder)\n",
        "spectral_encoder = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(input_img)\n",
        "spectral_encoder = layers.BatchNormalization()(spectral_encoder)\n",
        "spectral_encoder = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(spectral_encoder)\n",
        "spectral_encoder = layers.BatchNormalization()(spectral_encoder)\n",
        "spectral_encoder = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(spectral_encoder)\n",
        "spectral_encoder = layers.BatchNormalization()(spectral_encoder)\n",
        "forwardSpectralFlattened = layers.Flatten()(spectral_encoder)\n",
        "# Calculate the number of features after the convolutional layers\n",
        "num_features = 64 * 64 * 64        # Adjust this based on the output shape of the convolutional layers\n",
        "forwardSpectral = layers.Reshape((64, 64, num_features // (64 * 64)))(forwardSpectralFlattened)\n",
        "attention = layers.Attention()([forwardSpectral, forwardSpectral, forwardSpectral])\n",
        "forwardSpectral = layers.Conv2D(7, (3, 3), activation='PReLU', padding='same')(attention)\n",
        "print(\"forwardSpectral\", forwardSpectral.shape)\n",
        "\n",
        "##################################################################\n",
        "#spatial encoder path for spatial feature extraction\n",
        "forwardspatial = input_img\n",
        "print (\"\\nforwardspatial before: \", forwardspatial.shape)\n",
        "spatial_encoder = layers.Conv2D(256, (3, 3), activation='PReLU', padding='same')(forwardspatial)\n",
        "forwardspatial = layers.BatchNormalization()(spatial_encoder)\n",
        "spatial_encoder = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(forwardspatial)\n",
        "forwardspatial = layers.BatchNormalization()(spatial_encoder)\n",
        "forwardspatial = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(forwardspatial)\n",
        "forwardspatial = layers.BatchNormalization()(forwardspatial)\n",
        "forwardspatial = layers.Conv2D(32, (3, 3), activation='PReLU', padding='same')(forwardspatial)\n",
        "forwardspatial = layers.BatchNormalization()(forwardspatial)\n",
        "forwardspatial = layers.Conv2D(6, (3, 3), activation='PReLU', padding='same')(forwardspatial)\n",
        "print (\"forwardspatial after: \", forwardspatial.shape)\n",
        "\n",
        "###############################################################\n",
        "#Concatenate\n",
        "Concatenated = layers.Concatenate(axis = 3)([forwardSpectral, forwardspatial])\n",
        "print (\"\\nConcatenated : \",Concatenated.shape)\n",
        "\n",
        "###############################################################################\n",
        "#encoder Path cont.\n",
        "encoded = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(Concatenated)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "encoded = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(encoded)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "encoded = layers.Conv2D(32, (3, 3), activation='PReLU', padding='same')(encoded)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "###############################################################################\n",
        "#decoder Path\n",
        "\n",
        "decoded = layers.Conv2D(32, (3, 3), activation='PReLU', padding='same')(encoded)\n",
        "decoded = layers.UpSampling2D((2, 2))(decoded)\n",
        "\n",
        "decoded = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(decoded)\n",
        "decoded = layers.UpSampling2D((2, 2))(decoded)\n",
        "\n",
        "decoded = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(decoded)\n",
        "decoded = layers.UpSampling2D((2, 2))(decoded)\n",
        "decoded = layers.Conv2D(13, (3, 3), activation='PReLU', padding='same')(decoded)\n",
        "print (\"decoded : \",decoded.shape)\n",
        "\n",
        "###########################################\n",
        "# Spectral decoder path with attention mechanism\n",
        "\n",
        "# Spectral decoder path with attention mechanism\n",
        "Spectral_decoder = layers.Conv2D(256, (3, 3), activation='PReLU', padding='same')(decoded)\n",
        "Spectral_decoder = layers.BatchNormalization()(Spectral_decoder)\n",
        "Spectral_decoder = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(decoded)\n",
        "Spectral_decoder = layers.BatchNormalization()(Spectral_decoder)\n",
        "Spectral_decoder = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(Spectral_decoder)\n",
        "Spectral_decoder = layers.BatchNormalization()(Spectral_decoder)\n",
        "Spectral_decoder = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(Spectral_decoder)\n",
        "Spectral_decoder = layers.BatchNormalization()(Spectral_decoder)\n",
        "\n",
        "# Flatten the Spectral_decoder output\n",
        "Spectral_decoder_flattened = layers.Flatten()(Spectral_decoder)\n",
        "\n",
        "# Calculate the number of features after the convolutional layers\n",
        "num_features = 64 * 64 * 64        # Adjust this based on the output shape of the convolutional layers\n",
        "Spectral_decoder_reshaped = layers.Reshape((64, 64, num_features // (64 * 64)))(Spectral_decoder_flattened)\n",
        "\n",
        "\n",
        "# Apply attention mechanism\n",
        "attention = layers.Attention()([Spectral_decoder_reshaped, Spectral_decoder_reshaped, Spectral_decoder_reshaped])\n",
        "\n",
        "# Apply convolutional layers\n",
        "backwardSpectral = layers.Conv2D(7, (3, 3), activation='PReLU', padding='same')(attention)\n",
        "\n",
        "# Print the shape for verification\n",
        "print(\"backwardSpectral shape:\", backwardSpectral.shape)\n",
        "\n",
        "\n",
        "#############################################\n",
        "#spatial decoder path\n",
        "backwardspatial = decoded\n",
        "print (\"\\nbackwardSacial before: \",backwardspatial.shape)\n",
        "backwardspatial = layers.Conv2D(32, (3, 3), activation='PReLU', padding='same')(backwardspatial)\n",
        "backwardspatial = layers.BatchNormalization()(backwardspatial)\n",
        "backwardspatial = layers.Conv2D(64, (3, 3), activation='PReLU', padding='same')(backwardspatial)\n",
        "backwardspatial = layers.BatchNormalization()(backwardspatial)\n",
        "backwardspatial = layers.Conv2D(128, (3, 3), activation='PReLU', padding='same')(backwardspatial)\n",
        "backwardspatial = layers.BatchNormalization()(backwardspatial)\n",
        "backwardspatial = layers.Conv2D(256, (3, 3), activation='PReLU', padding='same')(backwardspatial)\n",
        "backwardspatial = layers.BatchNormalization()(backwardspatial)\n",
        "backwardspatial = layers.Conv2D(6, (3, 3), activation='PReLU', padding='same')(backwardspatial)\n",
        "print (\"backwardSacial after: \",backwardspatial.shape)\n",
        "\n",
        "output = layers.Concatenate(axis = 3)([backwardSpectral, backwardspatial])\n",
        "print(\"\\noutput: \", output.shape)\n",
        "\n",
        "autoencoder = keras.Model(input_img, output)\n",
        "\n",
        "autoencoder.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
        "                   loss='mse', metrics=['accuracy'])\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "# Load the saved weights into the model\n",
        "autoencoder.load_weights(weights_path)\n",
        "\n",
        "!pip install pydot graphviz\n",
        "import pydot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(autoencoder, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "from IPython.display import Image\n",
        "Image(filename='model.png')\n",
        "\n",
        "\n",
        "#Train using train and validation sets\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                epochs=200,\n",
        "                batch_size=16,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_val, x_val),\n",
        "                verbose = 1,\n",
        "                #callbacks=[tensorboard_callback]\n",
        "                )\n",
        "\n",
        "#plot loss and accuracy\n",
        "plt.figure(1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"MSE\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','test'], loc = 'upper right')\n",
        "plt.show\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','test'], loc = 'lower right')\n",
        "plt.show\n",
        "\n",
        "\n",
        "# Import the 'os' module\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# Create the directory if it doesn't exist\n",
        "weights_dir = '/content/gdrive/MyDrive/badr13_weights'  # Replace 'path/to/directory' with the desired directory path\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.makedirs(weights_dir)\n",
        "\n",
        "# Save the weights to the specified directory in the Spider environment\n",
        "weights_path = os.path.join(weights_dir, 'hybrid_depth_CNN_ATTENTION_multispectral_weights.h5')\n",
        "autoencoder.save_weights(weights_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Predict the recostructed images from testing data\n",
        "reconstructed = autoencoder.predict(x_test)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def normalize_image(image):\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    return image\n",
        "\n",
        "for i in range(0, 20):\n",
        "    normalized_x_test = normalize_image(x_test[i])\n",
        "    normalized_reconstructed = normalize_image(reconstructed[i])\n",
        "    test_value = PSNR(normalized_x_test, normalized_reconstructed)\n",
        "    print(f\"PSNR test value no. {i+1} = {test_value} dB\")\n",
        "for i in range (0,20):\n",
        "    ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    channel_axis=-1)\n",
        "    print(f\"SSIM SKIMAGE no. {i+1} = {ssim_skimage}\")\n",
        "for i in range (0,20):\n",
        "    mu_ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    win_size=11,\n",
        "    gaussian_weights=True,\n",
        "    sigma=1.5,\n",
        "    use_sample_covariance=False,\n",
        "    channel_axis=-1)  # Specify the channel axis\n",
        "    print(f\"Multiscale SSIM SKIMAGE no. {i+1} = {mu_ssim_skimage}\")\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"mse\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','val'], loc = 'upper right')\n",
        "plt.show\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','val'], loc = 'lower right')\n",
        "plt.show\n",
        "\n",
        "reconstructed = autoencoder.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "############ CALULATE THE INFERENCE TIME ##############\n",
        "# Load the image\n",
        "img = autoencoder.predict(np.expand_dims(x_test[0], axis=0)) #REPLACE WITH LOADED MODEL NAME\n",
        "reconstructed = []\n",
        "\n",
        "_,r,c,ch = img.shape\n",
        "img = np.reshape(img, (r,c,ch))\n",
        "reconstructed.append(img)\n",
        "reconstructed_array = np.array(reconstructed)\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.0  # Normalize if necessary\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = autoencoder.predict(img_array)\n",
        "end_time = time.time()\n",
        "\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# Calculate FLOPs for a Conv2D layer\n",
        "def calculate_conv2d_flops(layer):\n",
        "    kernel_height, kernel_width = layer.kernel_size\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    out_channels = layer.output_shape[-1]\n",
        "    flops_per_element = kernel_height * kernel_width * in_channels\n",
        "    flops_per_output = flops_per_element * out_channels\n",
        "    total_flops = flops_per_output * np.prod(layer.output_shape[1:-1])\n",
        "    return total_flops\n",
        "\n",
        "# Calculate MACs for a Conv2D layer\n",
        "def calculate_conv2d_macs(layer):\n",
        "    total_flops = calculate_conv2d_flops(layer)\n",
        "    return total_flops * 2  # Assuming one MAC for each FLOP\n",
        "\n",
        "# Calculate FLOPs for an LSTM layer\n",
        "def calculate_lstm_flops(layer):\n",
        "    units = layer.units\n",
        "    time_steps = layer.input_shape[1]\n",
        "    flops_per_input = units * (2 * layer.input_shape[-1] + units - 1)\n",
        "    total_flops = flops_per_input * time_steps\n",
        "    return total_flops\n",
        "\n",
        "# Calculate MACs for an LSTM layer\n",
        "def calculate_lstm_macs(layer):\n",
        "    total_flops = calculate_lstm_flops(layer)\n",
        "    return total_flops * 2  # Assuming one MAC for each FLOP\n",
        "\n",
        "# Calculate total FLOPs and MACs for the model\n",
        "total_flops = 0\n",
        "total_macs = 0\n",
        "\n",
        "for layer in autoencoder.layers:\n",
        "    if isinstance(layer, keras.layers.Conv2D):\n",
        "        total_flops += calculate_conv2d_flops(layer)\n",
        "        total_macs += calculate_conv2d_macs(layer)\n",
        "    elif isinstance(layer, keras.layers.LSTM):\n",
        "        total_flops += calculate_lstm_flops(layer)\n",
        "        total_macs += calculate_lstm_macs(layer)\n",
        "\n",
        "# Calculate computation time (randomly chosen value)\n",
        "computation_time = 0.001  # 1 ms\n",
        "\n",
        "# Calculate GFLOPS and GMAC\n",
        "gflops = (total_flops / inference_time) * 1e-9\n",
        "gmac = (total_macs / inference_time) * 1e-9\n",
        "\n",
        "print(\"Total GFLOPS:\", gflops)\n",
        "print(\"Total GMAC:\", gmac)\n",
        "\n",
        "reconstructed = autoencoder.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def normalize_image(image):\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    return image\n",
        "################################################################################\n",
        "for i in range(0, 20):\n",
        "    normalized_x_test = normalize_image(x_test[i])\n",
        "    normalized_reconstructed = normalize_image(reconstructed[i])\n",
        "    test_value = PSNR(normalized_x_test, normalized_reconstructed)\n",
        "    print(f\"PSNR test value no. {i+1} = {test_value} dB\")\n",
        "for i in range (0,20):\n",
        "    ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    channel_axis=-1)\n",
        "    print(f\"SSIM SKIMAGE no. {i+1} = {ssim_skimage}\")\n",
        "for i in range (0,20):\n",
        "    mu_ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    win_size=11,\n",
        "    gaussian_weights=True,\n",
        "    sigma=1.5,\n",
        "    use_sample_covariance=False,\n",
        "    channel_axis=-1)  # Specify the channel axis\n",
        "    print(f\"Multiscale SSIM SKIMAGE no. {i+1} = {mu_ssim_skimage}\")\n",
        "##################################################################\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "# Load your original and reconstructed multispectral images.\n",
        "# Ensure that they have the correct channel order and data range.\n",
        "\n",
        "# Normalize the images to the desired range (e.g., [0, 1]).\n",
        "normalized_original = (x_test[i] - x_test[i].min()) / (x_test[i].max() - x_test[i].min())\n",
        "normalized_reconstructed = (reconstructed[i] - reconstructed[i].min()) / (reconstructed[i].max() - reconstructed[i].min())\n",
        "\n",
        "# Initialize lists to store SSIM and MS-SSIM values for each channel.\n",
        "ssim_values = []\n",
        "ms_ssim_values = []\n",
        "\n",
        "# Calculate SSIM and MS-SSIM for each channel.\n",
        "for channel_idx in range(13):\n",
        "    ssim_channel = ssim(\n",
        "    normalized_original[..., 2],\n",
        "    normalized_reconstructed[..., 2],\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    channel_axis=-1)\n",
        "    ms_ssim_channel = ssim(\n",
        "    normalized_original[..., 2],\n",
        "    normalized_reconstructed[..., 2],\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    win_size=11,\n",
        "    gaussian_weights=True,\n",
        "    sigma=1.5,\n",
        "    use_sample_covariance=False,\n",
        "    channel_axis=-1)\n",
        "\n",
        "    ssim_values.append(ssim_channel)\n",
        "    ms_ssim_values.append(ms_ssim_channel)\n",
        "\n",
        "# Calculate the overall SSIM and MS-SSIM values.\n",
        "average_ssim = np.mean(ssim_values)\n",
        "average_ms_ssim = np.mean(ms_ssim_values)\n",
        "\n",
        "print(f\"Overall SSIM: {average_ssim}\")\n",
        "print(f\"Overall MS-SSIM: {average_ms_ssim}\")\n",
        "###########################################################################\n",
        "\n",
        "# Initialize lists to store quality scores\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "mu_ssim_scores = []\n",
        "\n",
        "for i in range(20):\n",
        "    normalized_x_test = normalize_image(x_test[i])\n",
        "    normalized_reconstructed = normalize_image(reconstructed[i])\n",
        "\n",
        "    # Calculate PSNR for the pair of images\n",
        "    psnr = PSNR(normalized_x_test, normalized_reconstructed)\n",
        "    psnr_scores.append(psnr)\n",
        "\n",
        "    # Calculate SSIM using skimage\n",
        "    ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    channel_axis=-1)\n",
        "    ssim_scores.append(ssim_skimage)\n",
        "\n",
        "    # Calculate Multiscale SSIM using skimage\n",
        "    mu_ssim_skimage = ssim(\n",
        "    normalized_x_test,\n",
        "    normalized_reconstructed,\n",
        "    data_range=x_test.max() - x_test.min(),\n",
        "    win_size=11,\n",
        "    gaussian_weights=True,\n",
        "    sigma=1.5,\n",
        "    use_sample_covariance=False,\n",
        "    channel_axis=-1)\n",
        "    mu_ssim_scores.append(mu_ssim_skimage)\n",
        "\n",
        "# Calculate the average scores\n",
        "average_psnr = np.mean(psnr_scores)\n",
        "average_ssim = np.mean(ssim_scores)\n",
        "average_mu_ssim = np.mean(mu_ssim_scores)\n",
        "\n",
        "# Print the average scores\n",
        "print(f\"Average PSNR: {average_psnr} dB\")\n",
        "print(f\"Average SSIM: {average_ssim}\")\n",
        "print(f\"Average Multiscale SSIM: {average_mu_ssim}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(0,20):\n",
        "    # Display grayscale images\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(7, 7))\n",
        "    axes[0].imshow(x_test[i][:, :, 7], cmap='gray')\n",
        "    axes[0].set_title(f\"Original Image {i+1}\")\n",
        "    axes[1].imshow(reconstructed[i][:, :, 7], cmap='gray')\n",
        "    axes[1].set_title(f\"Reconstructed Image {i+1}\")\n",
        "    plt.show()\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Display RGB composite images\n",
        "    test_image = x_test[i][:, :, [2, 3, 7]]  # Create an RGB composite using bands 4, 2, and 1\n",
        "    reconstructed_image = reconstructed[i][:, :, [2, 3, 7]]  # Create an RGB composite using bands 4, 2, and 1\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(5, 5))\n",
        "    axes[0].imshow(test_image)\n",
        "    axes[0].set_title(f\"Original Image {i+1}\")\n",
        "    axes[1].imshow(reconstructed_image)\n",
        "    axes[1].set_title(f\"Reconstructed Image {i+1}\")\n",
        "    plt.show()\n",
        "\n",
        "for i in range (0,20):\n",
        "        fig = plt.figure(figsize=(7, 7))\n",
        "        fig.add_subplot(1, 2,1)\n",
        "        plt.imshow(x_test[i][:, :, 7], cmap='gray')\n",
        "        fig.add_subplot(1, 2,2)\n",
        "        plt.imshow(reconstructed[i][:, :, 7], cmap='gray')\n",
        "        print(f\"image no. {i+1} \")\n",
        "        plt.show()\n",
        "\n",
        "for i in range (0,5):\n",
        "        fig = plt.figure(figsize=(6, 6))\n",
        "        fig.add_subplot(1, 2,1)\n",
        "        test_image = x_test[i][:, :, [2, 3, 7]]  # Create an RGB composite using bands 4, 2, and 1\n",
        "        plt.imshow(test_image)\n",
        "        fig.add_subplot(1, 2,2)\n",
        "        reconstructed_image = reconstructed[i][:, :, [2, 3, 7]]  # Create an RGB composite using bands 4, 2, and 1\n",
        "        plt.imshow(reconstructed_image)\n",
        "        print(f\"image no. {i+1} \")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "######################################################################\n",
        "# Training loop\n",
        "epochs = 1  # Number of training epochs\n",
        "batch_size = 64  # Batch size for training\n",
        "\n",
        "# Create arrays to store loss values for plotting\n",
        "history = {'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle the training data at the beginning of each epoch\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    trainInput_shuffled = x_train[indices]\n",
        "\n",
        "    # Perform training using model.fit\n",
        "    training_history = autoencoder.fit(\n",
        "        trainInput_shuffled,\n",
        "        trainInput_shuffled,  # Autoencoder has the same input and output\n",
        "        epochs=1,  # Train for one epoch\n",
        "        batch_size=64,\n",
        "        verbose=1,\n",
        "        validation_data=(x_val, x_val)  # Validation data also has the same input and output\n",
        "        )\n",
        "\n",
        "    # Append the loss values to the history dictionary\n",
        "    history['loss'].extend(training_history.history['loss'])\n",
        "    history['val_loss'].extend(training_history.history['val_loss'])\n",
        "\n",
        "    # At the end of each epoch, save and display a grid of images\n",
        "    if (epoch + 1) % 1 == 0:  # Save and display images every epoch\n",
        "        # Generate reconstructed images\n",
        "        reconstructed_images = autoencoder.predict(x_test, batch_size=64)\n",
        "\n",
        "        # Create a grid of images (8x8)\n",
        "        rows = 8\n",
        "        cols = 8\n",
        "        fig, axs = plt.subplots(rows, cols * 2, figsize=(12, 12))\n",
        "\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                index = i * rows + j\n",
        "                if index < len(x_test):\n",
        "                    # Display original x_test image\n",
        "                    original_image = x_test[index]  # Adjust shape as needed\n",
        "                    axs[i, j].imshow(original_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                    axs[i, j].axis('off')\n",
        "\n",
        "                    # Display reconstructed image\n",
        "                    reconstructed_image = reconstructed_images[index]  # Adjust shape as needed\n",
        "                    axs[i, j + cols].imshow(reconstructed_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                    axs[i, j + cols].axis('off')\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        plt.suptitle(f'Original vs. Reconstructed Images (Epoch {epoch + 1})')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 1  # Number of training epochs\n",
        "batch_size = 64  # Batch size for training\n",
        "\n",
        "# Create arrays to store loss values for plotting\n",
        "history = {'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle the training data at the beginning of each epoch\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    trainInput_shuffled = x_train[indices]\n",
        "\n",
        "    # Perform training using model.fit\n",
        "    training_history = autoencoder.fit(\n",
        "        trainInput_shuffled,\n",
        "        trainInput_shuffled,  # Autoencoder has the same input and output\n",
        "        epochs=1,  # Train for one epoch\n",
        "        batch_size=64,\n",
        "        verbose=1,\n",
        "        validation_data=(x_val, x_val)  # Validation data also has the same input and output\n",
        "    )\n",
        "\n",
        "    # Append the loss values to the history dictionary\n",
        "    history['loss'].extend(training_history.history['loss'])\n",
        "    history['val_loss'].extend(training_history.history['val_loss'])\n",
        "\n",
        "    # At the end of each epoch, save and display a grid of images\n",
        "    if (epoch + 1) % 1 == 0:  # Save and display images every epoch\n",
        "        # Generate reconstructed images\n",
        "        reconstructed_images = autoencoder.predict(x_val, batch_size=64)\n",
        "\n",
        "        # Create a grid of images (8x8)\n",
        "        rows = 8\n",
        "        cols = 8\n",
        "        fig, axs = plt.subplots(rows, cols, figsize=(12, 12))\n",
        "\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                index = i * rows + j\n",
        "                if index < len(reconstructed_images):\n",
        "                    # Display reconstructed image\n",
        "                    reconstructed_image = reconstructed_images[index]  # Adjust shape as needed\n",
        "                    import matplotlib.pyplot as plt\n",
        "\n",
        "                    # Assuming reconstructed_image is of shape (64, 64, 13)\n",
        "                    for channel in range(13):\n",
        "                       plt.subplot(2, 7, channel + 1)  # Adjust rows and columns as needed\n",
        "                       plt.imshow(reconstructed_image[:, :, channel], cmap='gray')  # Display individual channel\n",
        "                       plt.axis('off')\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        plt.suptitle(f'Reconstructed Images (Epoch {epoch + 1})')\n",
        "        plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "######################################################################\n",
        "# Training loop\n",
        "epochs = 1  # Number of training epochs\n",
        "batch_size = 64  # Batch size for training\n",
        "\n",
        "# Create arrays to store loss values for plotting\n",
        "history = {'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle the training data at the beginning of each epoch\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    trainInput_shuffled = x_train[indices]\n",
        "\n",
        "    # Perform training using model.fit\n",
        "    training_history = autoencoder.fit(\n",
        "        trainInput_shuffled,\n",
        "        trainInput_shuffled,  # Autoencoder has the same input and output\n",
        "        epochs=1,  # Train for one epoch\n",
        "        batch_size=64,\n",
        "        verbose=1,\n",
        "        validation_data=(x_val, x_val)  # Validation data also has the same input and output\n",
        "        )\n",
        "\n",
        "    # Append the loss values to the history dictionary\n",
        "    history['loss'].extend(training_history.history['loss'])\n",
        "    history['val_loss'].extend(training_history.history['val_loss'])\n",
        "\n",
        "    # At the end of each epoch, save and display a grid of images\n",
        "    if (epoch + 1) % 1 == 0:  # Save and display images every epoch\n",
        "        # Generate reconstructed images\n",
        "        reconstructed_images = autoencoder.predict(x_test, batch_size=64)\n",
        "\n",
        "        # Create a grid of images (8x8)\n",
        "        rows = 8\n",
        "        cols = 8\n",
        "        fig, axs = plt.subplots(rows, cols * 2, figsize=(12, 12))\n",
        "\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                index = i * rows + j\n",
        "                if index < len(x_test):\n",
        "                    # Display original x_test image\n",
        "                    original_image = x_test[index]  # Adjust shape as needed\n",
        "                    axs[i, j].imshow(original_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                    axs[i, j].axis('off')\n",
        "\n",
        "                    # Display reconstructed image\n",
        "                    reconstructed_image = reconstructed_images[index]  # Adjust shape as needed\n",
        "                    axs[i, j + cols].imshow(reconstructed_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                    axs[i, j + cols].axis('off')\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.09, hspace=0.09)\n",
        "        plt.suptitle(f'Original vs. Reconstructed Images (Epoch {epoch + 1})')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "unsJsuOTO19O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (epoch + 1) % 1 == 0:  # Save and display images every epoch\n",
        "        # Generate reconstructed images for the entire x_test\n",
        "        reconstructed_images = autoencoder.predict(x_test, batch_size=16)\n",
        "\n",
        "        # Create a grid of images (8x8) for both x_test and reconstructed images\n",
        "        rows = 8\n",
        "        cols = 8\n",
        "        fig, axs = plt.subplots(rows*2, cols, figsize=(cols, rows*2))\n",
        "\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                # Display original x_test image\n",
        "                original_image = x_test[i * cols + j]  # Adjust shape as needed\n",
        "                axs[i * 2, j].imshow(original_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                axs[i * 2, j].axis('off')\n",
        "\n",
        "                # Display corresponding reconstructed image\n",
        "                reconstructed_image = reconstructed_images[i * cols + j]  # Adjust shape as needed\n",
        "                axs[i * 2 + 1, j].imshow(reconstructed_image[:, :, 7], cmap='gray')  # Display one channel for example\n",
        "                axs[i * 2 + 1, j].axis('off')\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        plt.suptitle(f'Original vs. Reconstructed Images (Epoch {epoch + 1})')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "rFaF8BxLPF-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "############ CALULATE THE INFERENCE TIME, GMAC & GFLOPS ##############\n",
        "# Load the image\n",
        "img = autoencoder.predict(np.expand_dims(x_test[0], axis=0))\n",
        "reconstructed = []\n",
        "\n",
        "_,r,c,ch = img.shape\n",
        "img = np.reshape(img, (r,c,ch))\n",
        "reconstructed.append(img)\n",
        "reconstructed_array = np.array(reconstructed)\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.0  # Normalize if necessary\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = autoencoder.predict(img_array)\n",
        "end_time = time.time()\n",
        "\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# Calculate FLOPs for a Conv2D layer\n",
        "def calculate_conv2d_flops(layer):\n",
        "    kernel_height, kernel_width = layer.kernel_size\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    out_channels = layer.output_shape[-1]\n",
        "    flops_per_element = kernel_height * kernel_width * in_channels\n",
        "    flops_per_output = flops_per_element * out_channels\n",
        "    total_flops = flops_per_output * np.prod(layer.output_shape[1:-1])\n",
        "    return total_flops\n",
        "\n",
        "# Calculate MACs for a Conv2D layer\n",
        "def calculate_conv2d_macs(layer):\n",
        "    total_flops = calculate_conv2d_flops(layer)\n",
        "    return total_flops * 2  # Assuming one MAC for each FLOP\n",
        "\n",
        "# Calculate FLOPs for an LSTM layer\n",
        "def calculate_lstm_flops(layer):\n",
        "    units = layer.units\n",
        "    time_steps = layer.input_shape[1]\n",
        "    flops_per_input = units * (2 * layer.input_shape[-1] + units - 1)\n",
        "    total_flops = flops_per_input * time_steps\n",
        "    return total_flops\n",
        "\n",
        "# Calculate MACs for an LSTM layer\n",
        "def calculate_lstm_macs(layer):\n",
        "    total_flops = calculate_lstm_flops(layer)\n",
        "    return total_flops * 2  # Assuming one MAC for each FLOP\n",
        "\n",
        "# Calculate total FLOPs and MACs for the model\n",
        "total_flops = 0\n",
        "total_macs = 0\n",
        "\n",
        "for layer in autoencoder.layers:\n",
        "    if isinstance(layer, keras.layers.Conv2D):\n",
        "        total_flops += calculate_conv2d_flops(layer)\n",
        "        total_macs += calculate_conv2d_macs(layer)\n",
        "    elif isinstance(layer, keras.layers.LSTM):\n",
        "        total_flops += calculate_lstm_flops(layer)\n",
        "        total_macs += calculate_lstm_macs(layer)\n",
        "\n",
        "# Calculate computation time (randomly chosen value)\n",
        "#computation_time = 0.001  # 1 ms\n",
        "\n",
        "# Calculate GFLOPS and GMAC\n",
        "gflops = (total_flops / inference_time) * 1e-9\n",
        "gmac = (total_macs / inference_time) * 1e-9\n",
        "\n",
        "print(\"Total GFLOPS:\", gflops)\n",
        "print(\"Total GMAC:\", gmac)\n",
        "\n",
        "\n",
        "!pip install pydot graphviz\n",
        "import pydot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(autoencoder, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "from IPython.display import Image\n",
        "Image(filename='model.png')\n"
      ],
      "metadata": {
        "id": "_puOraX0PVZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e43a0dd3be974735bccd7fe0883333da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8e72e79db544ed7ad80b92dea6d3068",
              "IPY_MODEL_71023e35c033447f82fc58a491b06632",
              "IPY_MODEL_aa8f9f76774b4f20be0dc46539e3e822"
            ],
            "layout": "IPY_MODEL_fc056bd451774ae0b84a5bb0eeb30098"
          }
        },
        "f8e72e79db544ed7ad80b92dea6d3068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1da7b4448a774b2ab80d736e85f46b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_e58b9957e5164565a91464b534cd6d50",
            "value": "Dl Completed...: 100%"
          }
        },
        "71023e35c033447f82fc58a491b06632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6ca18ec5124c39be67a6f504e4a10e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc2476b5c19941ada1a10af09eab8d07",
            "value": 1
          }
        },
        "aa8f9f76774b4f20be0dc46539e3e822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a5d5c658b547ae807d3b012862ed7a",
            "placeholder": "​",
            "style": "IPY_MODEL_6aa1526968b140e9ae84003f0226680c",
            "value": " 1/1 [02:54&lt;00:00, 114.35s/ url]"
          }
        },
        "fc056bd451774ae0b84a5bb0eeb30098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da7b4448a774b2ab80d736e85f46b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e58b9957e5164565a91464b534cd6d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf6ca18ec5124c39be67a6f504e4a10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dc2476b5c19941ada1a10af09eab8d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5a5d5c658b547ae807d3b012862ed7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa1526968b140e9ae84003f0226680c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d68ec7795fe4cac85c9a7cd1fdc875e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9abe5b7023bc4aa8b24f083725f0a6be",
              "IPY_MODEL_6435fa784312463f8940ba26c4701c76",
              "IPY_MODEL_e8b56274b78e485cbb430217d9aa06f1"
            ],
            "layout": "IPY_MODEL_65d27d5c5a09450bb39e4679ee7a0232"
          }
        },
        "9abe5b7023bc4aa8b24f083725f0a6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df32d6e647cc403fa97d566d4229bcf0",
            "placeholder": "​",
            "style": "IPY_MODEL_c60c3d84598743b3b1fb0ed75d655417",
            "value": "Dl Size...: 100%"
          }
        },
        "6435fa784312463f8940ba26c4701c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68b074151cc44118abdd0c040c82956",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_075def203cae4a61aafc55b8cd556228",
            "value": 1
          }
        },
        "e8b56274b78e485cbb430217d9aa06f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a67fe5d07c4bd99390985175ca84aa",
            "placeholder": "​",
            "style": "IPY_MODEL_b62f0f0b91644247b56fbd67c20ae58b",
            "value": " 1971/1971 [02:54&lt;00:00, 17.97 MiB/s]"
          }
        },
        "65d27d5c5a09450bb39e4679ee7a0232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df32d6e647cc403fa97d566d4229bcf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60c3d84598743b3b1fb0ed75d655417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e68b074151cc44118abdd0c040c82956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "075def203cae4a61aafc55b8cd556228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39a67fe5d07c4bd99390985175ca84aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62f0f0b91644247b56fbd67c20ae58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f923fa744fe4b93a17fc77b98ece0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adea7a9694614e9eac8b03f1564844b2",
              "IPY_MODEL_350689e7b27c434f9ffd0ea9b2e9b2e4",
              "IPY_MODEL_bacc3a75e2744e0a8d2d77f982fa4ddb"
            ],
            "layout": "IPY_MODEL_87150937f8d0490ea47e7ac611118ce5"
          }
        },
        "adea7a9694614e9eac8b03f1564844b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_361114d416bc4bbd832ea09078f5ac5a",
            "placeholder": "​",
            "style": "IPY_MODEL_de41e328fe5141d39ab2f802a3f1c618",
            "value": "Extraction completed...:  58%"
          }
        },
        "350689e7b27c434f9ffd0ea9b2e9b2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_995292da6a5744828e48cba9b724f767",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a32045ee42874f329cb09cbf4bd27e46",
            "value": 1
          }
        },
        "bacc3a75e2744e0a8d2d77f982fa4ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7584db6b27a04a73a20df696c7146cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_924be73919e34089a1ee317827a8a1b4",
            "value": " 15793/27000 [02:54&lt;00:03, 3343.22 file/s]"
          }
        },
        "87150937f8d0490ea47e7ac611118ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "361114d416bc4bbd832ea09078f5ac5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de41e328fe5141d39ab2f802a3f1c618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "995292da6a5744828e48cba9b724f767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a32045ee42874f329cb09cbf4bd27e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7584db6b27a04a73a20df696c7146cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "924be73919e34089a1ee317827a8a1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}